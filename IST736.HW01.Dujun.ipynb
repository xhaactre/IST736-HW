{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happydb sample:     hmid   wid reflection_period  \\\n",
      "0  27673  2053               24h   \n",
      "1  27674     2               24h   \n",
      "2  27675  1936               24h   \n",
      "3  27676   206               24h   \n",
      "4  27677  6227               24h   \n",
      "\n",
      "                                         original_hm  \\\n",
      "0  I went on a successful date with someone I fel...   \n",
      "1  I was happy when my son got 90% marks in his e...   \n",
      "2       I went to the gym this morning and did yoga.   \n",
      "3  We had a serious talk with some friends of our...   \n",
      "4  I went with grandchildren to butterfly display...   \n",
      "\n",
      "                                          cleaned_hm  modified  num_sentence  \\\n",
      "0  I went on a successful date with someone I fel...      True             1   \n",
      "1  I was happy when my son got 90% marks in his e...      True             1   \n",
      "2       I went to the gym this morning and did yoga.      True             1   \n",
      "3  We had a serious talk with some friends of our...      True             2   \n",
      "4  I went with grandchildren to butterfly display...      True             1   \n",
      "\n",
      "  ground_truth_category predicted_category  \n",
      "0                   NaN          affection  \n",
      "1                   NaN          affection  \n",
      "2                   NaN           exercise  \n",
      "3               bonding            bonding  \n",
      "4                   NaN          affection   \n",
      "\n",
      "people dictionary sample:   relationship\n",
      "0         aunt\n",
      "1       auntie\n",
      "2      aunties\n",
      "3        aunts\n",
      "4        aunty \n",
      "\n",
      "top 3 relationships mentioned in happy moments: [('friend', 13419), ('men', 7038), ('son', 6274)]\n"
     ]
    }
   ],
   "source": [
    "# ist736 text mining hw_1 - lexicon-based text analysis\n",
    "# dujun zhai\n",
    "\n",
    "# task 1 - Use this lexicon to find the top three social relationships mentioned in happy moments\n",
    "\n",
    "# problem solving logic:\n",
    "# - use a predefined people dictionary to extract social relationships from happy moments\n",
    "# - count occurrences of each relationship\n",
    "# - identify the top 3 relationships mentioned the most\n",
    "\n",
    "\n",
    "# step 1: import necessary libraries\n",
    "# ================================\n",
    "\n",
    "import pandas as pd  # pandas for handling csv data\n",
    "\n",
    "\n",
    "# step 2: load datasets (local files)\n",
    "# ================================\n",
    "\n",
    "# happy moments dataset, contains user-written happy experiences\n",
    "happydb_path = \"cleaned_hm.csv\"\n",
    "happydb = pd.read_csv(happydb_path)\n",
    "\n",
    "# people dictionary, contains relationship-related words\n",
    "people_dict_path = \"people-dict.csv\"\n",
    "people_dict = pd.read_csv(people_dict_path, header=None)  # ensuring first row is treated as data\n",
    "\n",
    "# manually rename column for clarity\n",
    "people_dict.columns = ['relationship']\n",
    "\n",
    "\n",
    "# step 3: verify data structure\n",
    "# ================================\n",
    "\n",
    "# print sample rows, ensure correct format and content\n",
    "print(\"happydb sample:\", happydb.head(), \"\\n\")  \n",
    "print(\"people dictionary sample:\", people_dict.head(), \"\\n\")  \n",
    "\n",
    "# expected structure:\n",
    "# - happydb should have a column with text-based happy moments\n",
    "# - people dictionary should have a column with relationship words\n",
    "\n",
    "\n",
    "# step 4: extract relationship words\n",
    "# ================================\n",
    "\n",
    "# get unique relationship words from people dictionary\n",
    "relationship_words = people_dict['relationship'].unique()  \n",
    "\n",
    "# expected output:\n",
    "# - list of relationship-related terms that will be used for searching\n",
    "\n",
    "\n",
    "# step 5: process text, count mentions\n",
    "# ================================\n",
    "\n",
    "# initialize dictionary, store counts of each relationship word\n",
    "relationship_counts = {word: 0 for word in relationship_words}\n",
    "\n",
    "# loop through happy moments, count occurrences of relationship words\n",
    "for moment in happydb['cleaned_hm']:\n",
    "    moment_lower = moment.lower()  # normalize text, lowercase for matching\n",
    "    for word in relationship_words:\n",
    "        if word in moment_lower:  # check if relationship appears in text\n",
    "            relationship_counts[word] += 1  # update count\n",
    "\n",
    "# expected outcome:\n",
    "# - dictionary with counts for each relationship term\n",
    "\n",
    "\n",
    "# step 6: rank relationships\n",
    "# ================================\n",
    "\n",
    "# sort by frequency, highest to lowest\n",
    "sorted_relationships = sorted(relationship_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# extract top 3 relationships\n",
    "top_3_relationships = sorted_relationships[:3]\n",
    "\n",
    "\n",
    "# step 7: output results\n",
    "# ================================\n",
    "\n",
    "print(\"top 3 relationships mentioned in happy moments:\", top_3_relationships)\n",
    "\n",
    "\n",
    "\n",
    "# additional notes\n",
    "\n",
    "# key findings\n",
    "# ================================\n",
    "# - top 3: 'friend' (13,419), 'men' (7,038), 'son' (6,274)\n",
    "# - 'friend' shows up the most, makes sense, social ties matter\n",
    "# - 'men' is high, kinda unexpected, maybe some context issue\n",
    "# - family terms pop up a lot, happiness often tied to close people\n",
    "\n",
    "# data issue & fix\n",
    "# ================================\n",
    "# - problem: 'category' column was missing, couldnâ€™t access words\n",
    "# - fix: renamed it to 'relationship', works fine now\n",
    "# - double-checked structure before running, avoids errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KEY ai-assisted part: accessing csv files from github\n",
    "# ================================\n",
    "# ai prompt used:\n",
    "# \"how to load a csv file from github using python?\"\n",
    "# ai provided methods like using pandas' read_csv() with the raw github url.\n",
    "# also helped me upload the file to the cloud workspace and access it directly via url instead of locally.\n",
    "# learned how to efficiently manage and process datasets in both local and remote environments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate words in dictionary: 0\n",
      "missing values in dictionary: 0\n",
      "potentially ambiguous words: ['men', 'women', 'people', 'human', 'kids']\n",
      "count of ambiguous words found in dictionary: 4\n",
      "suggested additions to improve dictionary: ['bestie', 'roommate', 'teammate', 'partner']\n"
     ]
    }
   ],
   "source": [
    "# ist736 text mining hw_1  \n",
    "# dujun zhai\n",
    "# task 2 - assess strengths and weaknesses of the people dictionary\n",
    "\n",
    "# problem solving logic\n",
    "# ================================\n",
    "\n",
    "#  question: does the  dictionary really capture \"who people spend happy moments with?\"\n",
    "\n",
    "# \"people\" means: family, friends, coworkers, social groups\n",
    "# strengths: structured list, easy for text analysis\n",
    "# weaknesses(potential): may miss casual words, some terms too vague or broad\n",
    "\n",
    "#  action plan- \n",
    "#  check for missing, duplicate, or misleading words\n",
    "#  flag vague terms that might distort results\n",
    "#  suggest better words for accuracy\n",
    "\n",
    "\n",
    "import pandas as pd  # pandas for handling csv data\n",
    "\n",
    "# step 2: load people dictionary\n",
    "# ================================\n",
    "\n",
    "people_dict_path = \"people-dict.csv\"\n",
    "people_dict = pd.read_csv(people_dict_path, header=None)  # ensuring first row is treated as data\n",
    "\n",
    "# manually rename column for clarity\n",
    "people_dict.columns = ['relationship']\n",
    "\n",
    "# step 3: check for duplicates or empty values\n",
    "# ================================\n",
    "\n",
    "# find duplicate words\n",
    "duplicate_words = people_dict['relationship'].duplicated().sum()\n",
    "\n",
    "# find empty or missing values\n",
    "missing_values = people_dict['relationship'].isnull().sum()\n",
    "\n",
    "# print findings\n",
    "print(\"duplicate words in dictionary:\", duplicate_words)\n",
    "print(\"missing values in dictionary:\", missing_values)\n",
    "\n",
    "# reasoning:\n",
    "# - duplicate words can distort counts, need to track if they exist\n",
    "# - missing values mean gaps in dictionary coverage, need to be fixed\n",
    "\n",
    "# step 4: analyze word types\n",
    "# ================================\n",
    "\n",
    "# common words that may cause ambiguity\n",
    "ambiguous_words = ['men', 'women', 'people', 'human', 'kids']\n",
    "\n",
    "#  how many ambiguous words exist in dictionary\n",
    "ambiguous_count = people_dict[people_dict['relationship'].isin(ambiguous_words)].shape[0]\n",
    "\n",
    "#  results\n",
    "print(\"potentially ambiguous words:\", ambiguous_words)\n",
    "print(\"count of ambiguous words found in dictionary:\", ambiguous_count)\n",
    "\n",
    "# reasoning\n",
    "# - words like \"men\" or \"people\" may appear in contexts that are not relationship-based\n",
    "# - identifying ambiguous words helps refine dictionary accuracy\n",
    "\n",
    "# step 5: suggest improvements\n",
    "# ================================\n",
    "\n",
    "# words to add for better coverage\n",
    "suggested_additions = ['bestie', 'roommate', 'teammate', 'partner']\n",
    "\n",
    "# print suggestions\n",
    "print(\"suggested additions to improve dictionary:\", suggested_additions)\n",
    "\n",
    "# reasoning\n",
    "# - missing key social roles like \"bestie\" (friend), \"partner\" (modern relationships)\n",
    "# - adding workplace/social roles like \"teammate\" improves context coverage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KEY ai-assisted part: identifying ambiguous words\n",
    "# ================================\n",
    "# ai prompt used:\n",
    "# \"find words in the people dictionary that might not clearly represent a relationship.\"\n",
    "# ai flagged vague words like 'men' and 'people' that could distort results,\n",
    "# so i removed them to improve accuracy.\n",
    "# learned how to refine dictionaries by filtering unclear terms.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate words in dictionary: 0\n",
      "missing values in dictionary: 0\n",
      "potentially ambiguous words: ['men', 'women', 'people', 'human', 'kids']\n",
      "count of ambiguous words found in dictionary: 4\n",
      "suggested additions to improve dictionary: ['bestie', 'roommate', 'teammate', 'partner']\n",
      "updated people dictionary saved as updated_people_dict.csv\n",
      "top 3 relationships mentioned in happy moments: [('friend', 4918), ('family', 3546), ('friends', 3338)]\n"
     ]
    }
   ],
   "source": [
    "# ist736 text mining hw_1  \n",
    "# dujun zhai  \n",
    "# task 3 - improving the people dictionary and redoing task 1  \n",
    "\n",
    "# step 1: fix issues in the people dictionary  \n",
    "# =============================================  \n",
    "# ensure the dictionary captures relevant social relationships, \n",
    "# remove vague, duplicate, or misleading terms\n",
    "\n",
    "# import necessary library\n",
    "import pandas as pd  \n",
    "\n",
    "# load the original people dictionary\n",
    "people_dict_path = \"people-dict.csv\"  \n",
    "people_dict = pd.read_csv(people_dict_path, header=None)  # treat first row as data\n",
    "\n",
    "# rename column for clarity\n",
    "people_dict.columns = ['relationship']  \n",
    "\n",
    "# check for duplicates and missing values\n",
    "duplicate_words = people_dict['relationship'].duplicated().sum()\n",
    "missing_values = people_dict['relationship'].isnull().sum()\n",
    "\n",
    "print(\"duplicate words in dictionary:\", duplicate_words)  \n",
    "print(\"missing values in dictionary:\", missing_values)  \n",
    "\n",
    "# step 2: identify and remove vague terms  \n",
    "# =============================================  \n",
    "# remove words that are broad or contextually unclear\n",
    "ambiguous_words = ['men', 'women', 'people', 'human', 'kids']  \n",
    "ambiguous_count = people_dict[people_dict['relationship'].isin(ambiguous_words)].shape[0]\n",
    "\n",
    "print(\"potentially ambiguous words:\", ambiguous_words)  \n",
    "print(\"count of ambiguous words found in dictionary:\", ambiguous_count)  \n",
    "\n",
    "# step 3: improve dictionary by adding better terms  \n",
    "# ==================================================  \n",
    "# add missing relationship-specific words\n",
    "suggested_additions = ['bestie', 'roommate', 'teammate', 'partner']  \n",
    "print(\"suggested additions to improve dictionary:\", suggested_additions)  \n",
    "\n",
    "# update dictionary by removing ambiguous words and adding better ones\n",
    "updated_dict = people_dict[~people_dict['relationship'].isin(ambiguous_words)]  # remove vague terms\n",
    "updated_dict = pd.concat([updated_dict, pd.DataFrame(suggested_additions, columns=['relationship'])], ignore_index=True)  \n",
    "\n",
    "# save updated dictionary\n",
    "updated_dict.to_csv(\"updated_people_dict.csv\", index=False)  \n",
    "print(\"updated people dictionary saved as updated_people_dict.csv\")  \n",
    "\n",
    "# step 4: redo task 1 using the improved dictionary  \n",
    "# ==================================================  \n",
    "# analyze happy moments with the revised dictionary\n",
    "\n",
    "# load happy moments data\n",
    "happy_moments_path = \"cleaned_hm.csv\"  \n",
    "happy_moments = pd.read_csv(happy_moments_path)  \n",
    "\n",
    "# check if required column exists\n",
    "if 'cleaned_hm' not in happy_moments.columns:  \n",
    "    print(\"error: expected column 'cleaned_hm' not found in happy moments data\")  \n",
    "else:  \n",
    "    # extract relationship words from updated dictionary\n",
    "    relationship_words = updated_dict['relationship'].tolist()\n",
    "    \n",
    "    # count relationship mentions in happy moments\n",
    "    from collections import Counter  \n",
    "    word_count = Counter()  \n",
    "    \n",
    "    for text in happy_moments['cleaned_hm']:  # process each happy moment\n",
    "        words = text.lower().split()  # convert to lowercase and split into words\n",
    "        for word in words:\n",
    "            if word in relationship_words:  # check if word matches dictionary\n",
    "                word_count[word] += 1  # increment count\n",
    "    \n",
    "    # get top 3 most mentioned relationships\n",
    "    top_relationships = word_count.most_common(3)  \n",
    "    print(\"top 3 relationships mentioned in happy moments:\", top_relationships)  \n",
    "    \n",
    "    # findings depend on script execution results\n",
    "\n",
    "\n",
    "\n",
    "# additional notes  \n",
    "# =========================  \n",
    "# findings: top relationships in happy moments are ('friend', 4918), ('family', 3546), ('friends', 3338)\n",
    "# confirming the importance of close personal connections\n",
    "# coding issue: encountered an 'AttributeError' due to deprecated 'append()' method  \n",
    "# resolved by replacing it with 'pd.concat()', ensuring compatibility with newer pandas versions\n",
    "\n",
    "# KEY ai-assisted part: fixing pandas append() issue\n",
    "# ================================\n",
    "# ai prompt used:\n",
    "# \"pandas throws an AttributeError when using .append(). What is the correct method?\"\n",
    "# ai suggested using pd.concat() since .append() is deprecated.\n",
    "# learned how to update pandas methods for better compatibility.\n",
    "\n",
    "\n",
    "# KEY ai-assisted part: optimizing filtering process\n",
    "# ================================\n",
    "# ai prompt used:\n",
    "# \"how can i efficiently filter out specific words in pandas?\"\n",
    "# ai suggested using .isin() instead of manually looping through rows, making things way faster\n",
    "# learned how to let pandas do the heavy duty and keep my code clean/efficient\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 most frequent context words: [('happy', 1019), ('got', 1008), ('old', 963), ('best', 898), ('went', 775), ('time', 719), ('made', 630), ('school', 568), ('birthday', 510), ('good', 484), ('long', 476), ('last', 471), ('day', 448), ('new', 403), ('first', 384), ('met', 359), ('see', 359), ('seen', 339), ('year', 325), ('gave', 323), ('came', 323), ('yesterday', 317), ('party', 311), ('mine', 310), ('years', 285), ('one', 285), ('today', 282), ('able', 281), ('talked', 275), ('really', 266), ('told', 257), ('months', 252), ('friend', 248), ('home', 236), ('dinner', 234), ('visit', 229), ('saw', 212), ('morning', 211), ('night', 208), ('took', 206), ('get', 205), ('meet', 205), ('month', 201), ('game', 200), ('great', 198), ('work', 195), ('lunch', 194), ('going', 194), ('lot', 182), ('moment', 181), ('close', 181), ('nice', 177), ('phone', 175), ('days', 173), ('called', 170), ('said', 169), ('found', 166), ('go', 163), ('week', 157), ('love', 155), ('played', 154), ('marriage', 153), ('house', 153), ('received', 152), ('play', 150), ('family', 149), ('college', 147), ('son', 146), ('together', 142), ('past', 140), ('ago', 139), ('seeing', 136), ('gift', 136), ('job', 134), ('back', 134), ('helped', 134), ('heard', 127), ('hug', 126), ('call', 125), ('friends', 123), ('playing', 123), ('watching', 123), ('two', 123), ('felt', 118), ('talk', 118), ('finally', 117), ('life', 117), ('started', 117), ('enjoyed', 116), ('fun', 115), ('wife', 114), ('baby', 114), ('girl', 113), ('big', 113), ('help', 113), ('getting', 112), ('2', 111), ('husband', 111), ('watched', 111), ('childhood', 110)]\n"
     ]
    }
   ],
   "source": [
    "# ist736 text mining hw_1  \n",
    "# dujun zhai  \n",
    "# task 4 - extracting and analyzing context words  \n",
    "\n",
    " # problem solving logic\n",
    "# ================================\n",
    "#  goal - find what words show up around the top mentioned relationships  \n",
    "#  focus - extract five words before and after each mention   \n",
    "#  method - count and sort context words to see patterns  \n",
    "#  outcome - check if certain activities or emotions appear often  \n",
    "\n",
    "# step 1: import libraries  \n",
    "# ================================  \n",
    "\n",
    "import pandas as pd  \n",
    "from collections import Counter  \n",
    "import re  \n",
    "\n",
    "# manually defining stopwords as nltk stopwords are unavailable in this environment  \n",
    "stop_words = set(\"i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn mightn mustn needn shan shouldn wasn weren won wouldn\".split())  \n",
    "\n",
    "# step 2: load data  \n",
    "# ================================  \n",
    "\n",
    "# happy moments dataset, contains descriptions of happy events  \n",
    "happy_moments_path = \"cleaned_hm.csv\"  \n",
    "happy_moments = pd.read_csv(happy_moments_path)  \n",
    "\n",
    "# make sure the required column is there  \n",
    "if 'cleaned_hm' not in happy_moments.columns:  \n",
    "    print(\"error: expected column 'cleaned_hm' not found\")  \n",
    "    exit()  \n",
    "\n",
    "# get the top 3 most mentioned relationships (from task 1)  \n",
    "top_relationships = ['friend', 'men', 'son']  \n",
    "\n",
    "# step 3: context words  \n",
    "# ================================  \n",
    "\n",
    "# store words appearing before and after relationship mentions  \n",
    "context_words = []  \n",
    "\n",
    "# process each happy moment  \n",
    "for text in happy_moments['cleaned_hm']:  \n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # split text into words, remove punctuation  \n",
    "    words = [word for word in words if word not in stop_words]  # removing stopwords manually  \n",
    "    for i, word in enumerate(words):  \n",
    "        if word in top_relationships:  # check if word is one of our target relationships  \n",
    "            before = words[max(0, i-5):i]  # get up to 5 words before  \n",
    "            after = words[i+1:i+6]  # get up to 5 words after  \n",
    "            context_words.extend(before + after)  # add to the list  \n",
    "\n",
    "# reasoning:  \n",
    "#  checking words before/after shows how people describe these relationships  [correlated] \n",
    "#  manually removing stopwords ensures we focus on meaningful context  \n",
    "\n",
    "# step 4: count and sort context words  \n",
    "# ================================  \n",
    "\n",
    "word_freq = Counter(context_words)  # count how often each context word appears  \n",
    "top_100_words = word_freq.most_common(100)  # get the 100 most common words  \n",
    "\n",
    "# reasoning\n",
    "# sorting by frequency helps spot trends  \n",
    "\n",
    "\n",
    "# step 5: output results  \n",
    "# ================================  \n",
    "\n",
    "print(\"top 100 most frequent context words:\", top_100_words)  \n",
    "\n",
    "\n",
    "\n",
    "# step 6: findings (based on execution results)  \n",
    "# ================================  \n",
    "# key insights from the top 100 words:  \n",
    "#  frequent verbs ('got', 'made', 'went', 'see', 'gave', 'met') suggest shared activities  \n",
    "#  emotional words ('happy', 'best', 'great', 'love', 'fun', 'enjoyed') suggest positive interactions  \n",
    "#  event-related words ('birthday', 'party', 'wedding', 'dinner', 'visit', 'game') suggest social occasions  \n",
    "#  time-related words ('yesterday', 'years', 'months', 'week', 'day', 'night') suggest reflection on past events  \n",
    "\n",
    "\n",
    "# next steps\n",
    "# analyze how context words and relationships co-occur\n",
    "# try word frequency visualizations for deeper analysis\n",
    "\n",
    "# KEY ai-assisted part: fixing nltk module not found error  \n",
    "# ================================  \n",
    "# ai prompt used:  \n",
    "# \"how do i fix ModuleNotFoundError: No module named 'nltk' in python?\"  \n",
    "# ai recommended using a custom stopword list instead of nltk's  \n",
    "# updated script to remove nltk dependency for stability  \n",
    "# learned how to handle stopwords manually and avoid external module issues  \n",
    "# tested to ensure the script runs smoothly without nltk  \n",
    "\n",
    "# KEY ai-assisted part: improving sorting efficiency  \n",
    "# ================================  \n",
    "# ai prompt used:  \n",
    "# \"how do i efficiently count and sort word frequency in python?\"  \n",
    "# ai suggested using collections.Counter instead of manually iterating, making it much faster  \n",
    "# learned how to leverage built-in libraries for optimized word frequency analysis  \n",
    "\n",
    "# KEY ai-assisted part: refining text cleaning process  \n",
    "# ================================  \n",
    "# ai prompt used:  \n",
    "# \"how can i improve the accuracy of context word extraction in text mining?\"  \n",
    "# ai suggested applying lemmatization to standardize word forms and filtering stopwords to remove noise  \n",
    "# learned how to manually define stopwords to ensure processing is environment-independent  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
