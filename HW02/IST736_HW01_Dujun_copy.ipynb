{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3toms3mdVovv"
      },
      "source": [
        "1.\tDownload the Kaggle Financial Sentiment Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJsOm628VhPF"
      },
      "outputs": [],
      "source": [
        "# s1: data collection - kaggle financial sentiment data\n",
        "\n",
        "# the dataset 'data.csv' is already downloaded using the professor's link,\n",
        "# and it's uploaded to google drive in the same folder as the jupyter notebook.\n",
        "# let's load it and check the first few rows to make sure it's ready for use.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# load the dataset from google drive\n",
        "file_path = '/content/drive/MyDrive/HW02/data.csv'  # update this path if necessary\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# display the first few rows to verify the data is loaded properly\n",
        "df.head()  # quick check to make sure we're good to go\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yHQ1DShBZVhE"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfkTe7GZVTp"
      },
      "source": [
        "2. Use a randomized sample of 80% data for training, and the rest 20% for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt7oBTYkRfyR"
      },
      "outputs": [],
      "source": [
        "# s2: split the data into training and testing sets (80% training, 20% testing)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X is the input data (the sentences) and y is the target labels (sentiments)\n",
        "X = df['Sentence']  # sentences (input)\n",
        "y = df['Sentiment']  # sentiment labels (output)\n",
        "\n",
        "# split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# check the size of the splits to ensure it's correct\n",
        "print(f\"Training data size: {len(X_train)}\")\n",
        "print(f\"Testing data size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxryLdCeMqM"
      },
      "source": [
        "3.Build a linearSVC classifier using unigrams. You can decide on the other vectorization options.\n",
        "\n",
        "a.\tReport the top 20 positive features and negative features.\n",
        "\n",
        "b.\tReport the f1 and accuracy results.\n",
        "\n",
        "c.\tExamine up to 25 FP and FN errors and report linguistic patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjnnE035eMTX"
      },
      "outputs": [],
      "source": [
        "# s3: build a LinearSVC classifier using unigrams\n",
        "\n",
        "# load necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define file path for the dataset in google drive\n",
        "file_path = '/content/drive/MyDrive/HW02/data.csv'  # file is already accessible\n",
        "\n",
        "# load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# separate features (X) and labels (y)\n",
        "X = df['Sentence']  # input text\n",
        "y = df['Sentiment']  # sentiment labels\n",
        "\n",
        "# split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert text into numerical format using tf-idf (unigrams only)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1))  # unigrams only\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)  # fit and transform training data\n",
        "X_test_tfidf = vectorizer.transform(X_test)  # transform test data (same vectorizer)\n",
        "\n",
        "# train the LinearSVC model\n",
        "svc = LinearSVC(random_state=42, dual=False)  # linear support vector classifier, dual=False for efficiency\n",
        "svc.fit(X_train_tfidf, y_train)  # train on labeled data\n",
        "\n",
        "# extract feature names\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())  # get all feature names\n",
        "\n",
        "# get the mean absolute value of the coefficients (for feature importance)\n",
        "coefficients = np.mean(np.abs(svc.coef_), axis=0)\n",
        "\n",
        "# get the top 20 features that contribute most to positive/negative classification\n",
        "top_positive_idx = coefficients.argsort()[-20:][::-1]  # highest positive weights\n",
        "top_negative_idx = coefficients.argsort()[:20]  # lowest (most negative) weights\n",
        "\n",
        "# get the actual words\n",
        "top_positive_features = feature_names[top_positive_idx]\n",
        "top_negative_features = feature_names[top_negative_idx]\n",
        "\n",
        "# s3a: visualize top 20 positive & negative features (black/gray bars)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# positive features bar chart (black bars)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(top_positive_features[::-1], coefficients[top_positive_idx][::-1], color='black')\n",
        "plt.xlabel(\"importance score\")\n",
        "plt.title(\"top 20 positive features\")\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# negative features bar chart (gray bars)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.barh(top_negative_features[::-1], coefficients[top_negative_idx][::-1], color='gray')\n",
        "plt.xlabel(\"importance score\")\n",
        "plt.title(\"top 20 negative features\")\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# s3b: model performance (using a table, no visualization)\n",
        "y_pred = svc.predict(X_test_tfidf)  # make predictions on test data\n",
        "\n",
        "# compute accuracy and f1 score\n",
        "accuracy = accuracy_score(y_test, y_pred)  # percentage of correct predictions\n",
        "report = classification_report(y_test, y_pred, output_dict=True)  # full classification report\n",
        "f1_score = report['accuracy']  # extract accuracy score\n",
        "\n",
        "# display model performance as a table\n",
        "evaluation_df = pd.DataFrame({\n",
        "    \"metric\": [\"accuracy\", \"f1 score\"],\n",
        "    \"score\": [accuracy, f1_score]\n",
        "})\n",
        "print(\"\\nmodel performance metrics:\")\n",
        "print(evaluation_df)\n",
        "\n",
        "# s3c: analyze false positives (FP) and false negatives (FN)\n",
        "false_positives = []  # predicted positive, but should not be\n",
        "false_negatives = []  # predicted negative, but should not be\n",
        "\n",
        "# iterate through test data and collect errors\n",
        "for i in range(len(y_test)):\n",
        "    if y_pred[i] == 'positive' and y_test.iloc[i] != 'positive':\n",
        "        false_positives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))\n",
        "    elif y_pred[i] == 'negative' and y_test.iloc[i] != 'negative':\n",
        "        false_negatives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))\n",
        "\n",
        "# limit to first 25 examples of each\n",
        "num_fp_display = min(25, len(false_positives))\n",
        "num_fn_display = min(25, len(false_negatives))\n",
        "\n",
        "# create dataframe for fp & fn errors\n",
        "fp_fn_df = pd.DataFrame({\n",
        "    \"sentence\": [fp[0] for fp in false_positives[:num_fp_display]] + [fn[0] for fn in false_negatives[:num_fn_display]],\n",
        "    \"true label\": [fp[1] for fp in false_positives[:num_fp_display]] + [fn[1] for fn in false_negatives[:num_fn_display]],\n",
        "    \"predicted label\": [fp[2] for fp in false_positives[:num_fp_display]] + [fn[2] for fn in false_negatives[:num_fn_display]]\n",
        "})\n",
        "\n",
        "print(\"\\nfalse positives & false negatives:\")\n",
        "print(fp_fn_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okQSglCslwSc"
      },
      "source": [
        "# s4: build a logistic regression classifier using fasttext embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WczvhAJnRhZJ"
      },
      "outputs": [],
      "source": [
        "# s1: install fasttext, load dataset, and initialize embeddings\n",
        "\n",
        "# install fasttext (if not already installed)\n",
        "!pip install fasttext\n",
        "\n",
        "# load necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fasttext.util  # for loading pre-trained fasttext embeddings\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define file path for the dataset in google drive\n",
        "file_path = '/content/drive/MyDrive/HW02/data.csv'  # file is already accessible\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# separate features (X) and labels (y)\n",
        "X = df['Sentence']  # input text, raw sentences\n",
        "y = df['Sentiment']  # sentiment labels (positive, negative, neutral)\n",
        "\n",
        "# split data into 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# download and load pre-trained fasttext embeddings (english, 300-dimensional vectors)\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # download only if not already available\n",
        "ft = fasttext.load_model('cc.en.300.bin')  # load fasttext model\n",
        "\n",
        "# fasttext embeddings are now ready to be used for sentence vectorization\n",
        "print(\"dataset loaded, fasttext embeddings initialized\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
