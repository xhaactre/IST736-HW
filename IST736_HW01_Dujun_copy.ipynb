{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhaactre/IST736-HW/blob/main/IST736_HW01_Dujun_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "iY1Dxa3BvGeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDownload the Kaggle Financial Sentiment Data"
      ],
      "metadata": {
        "id": "3toms3mdVovv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s1: data collection - kaggle financial sentiment data\n",
        "\n",
        "# the dataset 'data.csv' is already downloaded using the professor's link,\n",
        "# and it's uploaded to google drive in the same folder as the jupyter notebook.\n",
        "# let's load it and check the first few rows to make sure it's ready for use.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# load the dataset from google drive\n",
        "file_path = '/content/drive/MyDrive/IST708HW02/data.csv'  # update this path if necessary\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# display the first few rows to verify the data is loaded properly\n",
        "df.head()  # quick check to make sure we're good to go\n"
      ],
      "metadata": {
        "id": "JJsOm628VhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use a randomized sample of 80% data for training, and the rest 20% for testing"
      ],
      "metadata": {
        "id": "HEfkTe7GZVTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt7oBTYkRfyR"
      },
      "outputs": [],
      "source": [
        "# s2: split the data into training and testing sets (80% training, 20% testing)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X is the input data (the sentences) and y is the target labels (sentiments)\n",
        "X = df['Sentence']  # sentences (input)\n",
        "y = df['Sentiment']  # sentiment labels (output)\n",
        "\n",
        "# split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# check the size of the splits to ensure it's correct\n",
        "print(f\"Training data size: {len(X_train)}\")\n",
        "print(f\"Testing data size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Build a linearSVC classifier using unigrams. You can decide on the other vectorization options.\n",
        "\n",
        "a.\tReport the top 20 positive features and negative features.\n",
        "\n",
        "b.\tReport the f1 and accuracy results.\n",
        "\n",
        "c.\tExamine up to 25 FP and FN errors and report linguistic patterns.\n"
      ],
      "metadata": {
        "id": "ddxryLdCeMqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s3: build a LinearSVC classifier using unigrams\n",
        "\n",
        "# load necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define file path for the dataset in google drive\n",
        "file_path = '/content/drive/MyDrive/IST708HW02/data.csv'  # file is already accessible\n",
        "\n",
        "# load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# separate features (X) and labels (y)\n",
        "X = df['Sentence']  # input text\n",
        "y = df['Sentiment']  # sentiment labels\n",
        "\n",
        "# split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert text into numerical format using tf-idf (unigrams only)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1))  # unigrams only\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)  # fit and transform training data\n",
        "X_test_tfidf = vectorizer.transform(X_test)  # transform test data (same vectorizer)\n",
        "\n",
        "# train the LinearSVC model\n",
        "svc = LinearSVC(random_state=42, dual=False)  # linear support vector classifier, dual=False for efficiency\n",
        "svc.fit(X_train_tfidf, y_train)  # train on labeled data\n",
        "\n",
        "# extract feature names\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())  # get all feature names\n",
        "\n",
        "# get the mean absolute value of the coefficients (for feature importance)\n",
        "coefficients = np.mean(np.abs(svc.coef_), axis=0)\n",
        "\n",
        "# get the top 20 features that contribute most to positive/negative classification\n",
        "top_positive_idx = coefficients.argsort()[-20:][::-1]  # highest positive weights\n",
        "top_negative_idx = coefficients.argsort()[:20]  # lowest (most negative) weights\n",
        "\n",
        "# get the actual words\n",
        "top_positive_features = feature_names[top_positive_idx]\n",
        "top_negative_features = feature_names[top_negative_idx]\n",
        "\n",
        "# s3a: visualize top 20 positive & negative features (black/gray bars)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# positive features bar chart (black bars)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(top_positive_features[::-1], coefficients[top_positive_idx][::-1], color='black')\n",
        "plt.xlabel(\"importance score\")\n",
        "plt.title(\"top 20 positive features\")\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# negative features bar chart (gray bars)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.barh(top_negative_features[::-1], coefficients[top_negative_idx][::-1], color='gray')\n",
        "plt.xlabel(\"importance score\")\n",
        "plt.title(\"top 20 negative features\")\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# s3b: model performance (using a table, no visualization)\n",
        "y_pred = svc.predict(X_test_tfidf)  # make predictions on test data\n",
        "\n",
        "# compute accuracy and f1 score\n",
        "accuracy = accuracy_score(y_test, y_pred)  # percentage of correct predictions\n",
        "report = classification_report(y_test, y_pred, output_dict=True)  # full classification report\n",
        "f1_score = report['accuracy']  # extract accuracy score\n",
        "\n",
        "# display model performance as a table\n",
        "evaluation_df = pd.DataFrame({\n",
        "    \"metric\": [\"accuracy\", \"f1 score\"],\n",
        "    \"score\": [accuracy, f1_score]\n",
        "})\n",
        "print(\"\\nmodel performance metrics:\")\n",
        "print(evaluation_df)\n",
        "\n",
        "# s3c: analyze false positives (FP) and false negatives (FN)\n",
        "false_positives = []  # predicted positive, but should not be\n",
        "false_negatives = []  # predicted negative, but should not be\n",
        "\n",
        "# iterate through test data and collect errors\n",
        "for i in range(len(y_test)):\n",
        "    if y_pred[i] == 'positive' and y_test.iloc[i] != 'positive':\n",
        "        false_positives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))\n",
        "    elif y_pred[i] == 'negative' and y_test.iloc[i] != 'negative':\n",
        "        false_negatives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))\n",
        "\n",
        "# limit to first 25 examples of each\n",
        "num_fp_display = min(25, len(false_positives))\n",
        "num_fn_display = min(25, len(false_negatives))\n",
        "\n",
        "# create dataframe for fp & fn errors\n",
        "fp_fn_df = pd.DataFrame({\n",
        "    \"sentence\": [fp[0] for fp in false_positives[:num_fp_display]] + [fn[0] for fn in false_negatives[:num_fn_display]],\n",
        "    \"true label\": [fp[1] for fp in false_positives[:num_fp_display]] + [fn[1] for fn in false_negatives[:num_fn_display]],\n",
        "    \"predicted label\": [fp[2] for fp in false_positives[:num_fp_display]] + [fn[2] for fn in false_negatives[:num_fn_display]]\n",
        "})\n",
        "\n",
        "print(\"\\nfalse positives & false negatives:\")\n",
        "print(fp_fn_df)\n"
      ],
      "metadata": {
        "id": "mjnnE035eMTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# s4: build a logistic regression classifier using fasttext embeddings"
      ],
      "metadata": {
        "id": "okQSglCslwSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s1: load dataset and download fasttext embeddings\n",
        "\n",
        "# install fasttext (if not already installed)\n",
        "!pip install fasttext\n",
        "\n",
        "# load necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fasttext.util  # for downloading and loading fasttext embeddings\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define file path for the dataset (UPDATED TO CORRECT DIRECTORY)\n",
        "file_path = '/content/drive/MyDrive/IST708HW02/data.csv'  # correct dataset path\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# separate features (X) and labels (y)\n",
        "X = df['Sentence']  # input text, raw sentences\n",
        "y = df['Sentiment']  # sentiment labels (positive, negative, neutral)\n",
        "\n",
        "# split data into 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# download and load pre-trained fasttext embeddings (english, 300-dimensional vectors)\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # download model if not already present\n",
        "ft = fasttext.load_model('cc.en.300.bin')  # load fasttext model from local storage\n",
        "\n",
        "# fasttext embeddings are now ready to be used for sentence vectorization\n",
        "print(\"dataset loaded, fasttext embeddings downloaded and initialized\")\n"
      ],
      "metadata": {
        "id": "WczvhAJnRhZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s2: convert sentences into fasttext embeddings (high-level breakdown)**\n",
        "\n",
        " before training logistic regression, sentences must be converted into numerical format.\n",
        "\n",
        "key steps:\n",
        "\n",
        "-split each sentence into words\n",
        "\n",
        "-retrieve the fasttext vector for each word (if available)\n",
        "\n",
        "-ignore words that are not in the fasttext vocabulary\n",
        "\n",
        "-average the word vectors to create a sentence vector\n",
        "\n",
        "-apply this process to all training and test sentences\n",
        "\n",
        "-this generates a meaningful numeric representation for each sentence, making it ready for classification"
      ],
      "metadata": {
        "id": "ViOypI2G0C9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s2: convert sentences into fasttext embeddings\n",
        "\n",
        "# function to convert a sentence into a fasttext vector\n",
        "def sentence_to_vector(sentence, model, embedding_dim=300):\n",
        "    words = sentence.split()  # split sentence into words\n",
        "    word_vectors = [model.get_word_vector(word) for word in words if word in model]  # get word vectors\n",
        "\n",
        "    if len(word_vectors) == 0:  # if none of the words are in fasttext vocab\n",
        "        return np.zeros(embedding_dim)  # return a zero vector for empty cases\n",
        "\n",
        "    return np.mean(word_vectors, axis=0)  # compute the average word vector\n",
        "\n",
        "# convert training and testing sentences into embeddings\n",
        "X_train_vectors = np.array([sentence_to_vector(sentence, ft) for sentence in X_train])\n",
        "X_test_vectors = np.array([sentence_to_vector(sentence, ft) for sentence in X_test])\n",
        "\n",
        "print(\"sentences converted into fasttext embeddings\")\n"
      ],
      "metadata": {
        "id": "NDj3sDngzkbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rERvbco82HFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s3: train logistic regression classifier (high-level breakdown)**\n",
        "\n",
        "\n",
        "sentences have been converted into fasttext embeddings, allowing a logistic regression classifier to be trained.\n",
        "\n",
        "key steps:\n",
        "\n",
        "-train a logistic regression model using the sentence embeddings\n",
        "\n",
        "-make predictions on the test data\n",
        "\n",
        "-evaluate performance using accuracy and f1 score\n",
        "identify false positives (FP) and false negatives (FN) to analyze errors\n",
        "\n",
        "-this will show how well fasttext embeddings perform for sentiment classification."
      ],
      "metadata": {
        "id": "xhUhsqUv2afM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s3: train logistic regression classifier\n",
        "\n",
        "# load logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# create the model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)  # increase max_iter to ensure convergence\n",
        "\n",
        "# train the model on sentence embeddings\n",
        "clf.fit(X_train_vectors, y_train)  # learns the relationship between embeddings and sentiment labels\n",
        "\n",
        "# make predictions on test data\n",
        "y_pred = clf.predict(X_test_vectors)  # uses trained model to predict sentiment on test sentences\n",
        "\n",
        "# evaluate model performance (accuracy and f1 score)\n",
        "accuracy = accuracy_score(y_test, y_pred)  # calculates the percentage of correct predictions\n",
        "report = classification_report(y_test, y_pred, output_dict=True)  # generates full classification report\n",
        "f1_score = report['accuracy']  # extracts accuracy score\n",
        "\n",
        "# display model evaluation metrics\n",
        "print(\"\\nmodel performance:\")\n",
        "print(f\"accuracy: {accuracy:.4f}\")  # print accuracy with 4 decimal places\n",
        "print(f\"f1 score: {f1_score:.4f}\")  # print f1 score with 4 decimal places\n"
      ],
      "metadata": {
        "id": "Um5ltXlF0frb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s4: analyze false positives (FP) & false negatives (FN) (high-level breakdown)**\n",
        "\n",
        "-logistic regression has been trained, and accuracy has been evaluated. next, errors need to be analyzed.\n",
        "\n",
        "key steps:\n",
        "\n",
        "-identify false positives (FP) → sentences predicted as positive but should not be\n",
        "\n",
        "-identify false negatives (FN) → sentences predicted as negative but should not be\n",
        "\n",
        "-examine patterns in misclassified sentences → look for trends in mistakes\n",
        "\n",
        "-this helps pinpoint where the model struggles and provides insights for potential improvements."
      ],
      "metadata": {
        "id": "xgxYZDzC3NXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s4: analyze false positives (FP) & false negatives (FN)\n",
        "\n",
        "# initialize lists to store misclassified examples\n",
        "false_positives = []  # sentences wrongly classified as positive\n",
        "false_negatives = []  # sentences wrongly classified as negative\n",
        "\n",
        "# loop through test data to find misclassified cases\n",
        "for i in range(len(y_test)):\n",
        "    if y_pred[i] == 'positive' and y_test.iloc[i] != 'positive':\n",
        "        false_positives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))  # collect FP examples\n",
        "    elif y_pred[i] == 'negative' and y_test.iloc[i] != 'negative':\n",
        "        false_negatives.append((X_test.iloc[i], y_test.iloc[i], y_pred[i]))  # collect FN examples\n",
        "\n",
        "# limit display to first 25 examples in each category\n",
        "num_fp_display = min(25, len(false_positives))  # show up to 25 false positives\n",
        "num_fn_display = min(25, len(false_negatives))  # show up to 25 false negatives\n",
        "\n",
        "# create a dataframe for FP & FN results\n",
        "fp_fn_df = pd.DataFrame({\n",
        "    \"sentence\": [fp[0] for fp in false_positives[:num_fp_display]] + [fn[0] for fn in false_negatives[:num_fn_display]],\n",
        "    \"true label\": [fp[1] for fp in false_positives[:num_fp_display]] + [fn[1] for fn in false_negatives[:num_fn_display]],\n",
        "    \"predicted label\": [fp[2] for fp in false_positives[:num_fp_display]] + [fn[2] for fn in false_negatives[:num_fn_display]]\n",
        "})\n",
        "\n",
        "# print the misclassified sentences\n",
        "print(\"\\nfalse positives & false negatives:\")\n",
        "print(fp_fn_df)\n"
      ],
      "metadata": {
        "id": "PCsfUYrX39GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**some notes**\n",
        "\n",
        " the logistic regression model using fasttext embeddings achieved 68.09% accuracy, showing that fasttext captures word meaning but struggles with sentiment nuances.\n",
        "\n",
        " -findings:\n",
        "\n",
        "negative sentences misclassified as positive → likely due to words like \"profit\" appearing in a loss-related context.\n",
        "\n",
        "neutral sentences often misclassified → financial terms may not have strong sentiment signals, leading to misalignment.\n",
        "\n",
        "\n",
        "-model limitations:\n",
        "\n",
        "fasttext embeddings focus on word meaning, but they do not always capture sentiment intensity.\n",
        "\n",
        "sentiment in financial text is complex, and context matters more than individual word meaning.\n",
        "\n",
        "\n",
        "-possible improvements:\n",
        "\n",
        "fine-tune fasttext on finance-specific data for better domain adaptation.\n",
        "\n",
        "combine fasttext embeddings with tf-idf features to balance meaning and sentiment strength.\n",
        "\n",
        "-conclusion\n",
        "\n",
        "fasttext provides a strong foundation, but improvements are needed for more precise\n",
        "sentiment classification in financial contexts."
      ],
      "metadata": {
        "id": "B8bgjW-04hUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra points - Use fasttext supervised method to train a classifier using 80% labeled data**"
      ],
      "metadata": {
        "id": "4zPSq92e5Pk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "breakdown of the task: fasttext supervised classification & comparison\n",
        "this task involves training a supervised classifier using fasttext, with 80% of labeled data for training and 20% for testing. the evaluation will follow the same structure as previous classifiers (LinearSVC and Logistic Regression)."
      ],
      "metadata": {
        "id": "mL-nGzFG58Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s1: prepare dataset for fasttext**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each sentence must start with __label__ followed by its sentiment.\n",
        "\n",
        "Labels must be lowercase and contain no spaces.\n",
        "\n",
        "Data needs to be split into 80% training and 20% testing, saved as text files for FastText."
      ],
      "metadata": {
        "id": "lZKTt1Fx6Yx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s1: prepare dataset for fasttext\n",
        "\n",
        "# function to format sentences for fasttext\n",
        "def format_for_fasttext(sentence, label):\n",
        "    return f\"__label__{label} {sentence}\"\n",
        "\n",
        "# apply formatting to training and testing data\n",
        "train_data = [format_for_fasttext(X_train.iloc[i], y_train.iloc[i]) for i in range(len(X_train))]\n",
        "test_data = [format_for_fasttext(X_test.iloc[i], y_test.iloc[i]) for i in range(len(X_test))]\n",
        "\n",
        "# define file paths for fasttext\n",
        "train_file = \"/content/fasttext_train.txt\"\n",
        "test_file = \"/content/fasttext_test.txt\"\n",
        "\n",
        "# save formatted data to text files\n",
        "with open(train_file, \"w\") as f:\n",
        "    f.write(\"\\n\".join(train_data))\n",
        "\n",
        "with open(test_file, \"w\") as f:\n",
        "    f.write(\"\\n\".join(test_data))\n",
        "\n",
        "print(\"training and testing datasets formatted and saved for fasttext\")\n"
      ],
      "metadata": {
        "id": "DcmRx-Hw5lS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s2: train fasttext supervised model**\n",
        "\n",
        "\n",
        " FastText will now be trained as a supervised classifier using the formatted training data.\n",
        "\n",
        "The model will learn sentiment classification from the labeled data.\n",
        "\n",
        "After training, it will predict sentiment on the test dataset.\n",
        "\n",
        "Training time depends on dataset size, and if needed, a smaller sample can be used."
      ],
      "metadata": {
        "id": "MtHaJzCD6hZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s2: train fasttext supervised model\n",
        "\n",
        "# install fasttext if not installed\n",
        "!pip install fasttext\n",
        "\n",
        "# load fasttext\n",
        "import fasttext\n",
        "\n",
        "# define model parameters\n",
        "fasttext_model = fasttext.train_supervised(input=\"/content/fasttext_train.txt\", epoch=25, lr=0.5, wordNgrams=2, verbose=2)\n",
        "\n",
        "# save the trained model\n",
        "fasttext_model.save_model(\"/content/fasttext_model.bin\")\n",
        "\n",
        "print(\"fasttext model trained and saved\")\n"
      ],
      "metadata": {
        "id": "11Gmma176u5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s3: evaluate model performance**\n",
        "\n",
        "The trained FastText classifier is now ready for evaluation.\n",
        "\n",
        "The model will predict sentiment on the test dataset.\n",
        "\n",
        "Accuracy and F1-score will be measured (same as 3b).\n",
        "\n",
        "This will help determine how well FastText performs compared to the previous models."
      ],
      "metadata": {
        "id": "RoHyloGR6173"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s3: evaluate fasttext model performance\n",
        "\n",
        "# load the trained model\n",
        "fasttext_model = fasttext.load_model(\"/content/fasttext_model.bin\")\n",
        "\n",
        "# function to predict sentiment using fasttext\n",
        "def predict_fasttext(sentence, model):\n",
        "    label = model.predict(sentence)[0][0]  # get the predicted label\n",
        "    return label.replace(\"__label__\", \"\")  # remove fasttext label prefix\n",
        "\n",
        "# apply the model to test data\n",
        "y_pred_fasttext = [predict_fasttext(sentence, fasttext_model) for sentence in X_test]\n",
        "\n",
        "# calculate accuracy and f1-score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy_fasttext = accuracy_score(y_test, y_pred_fasttext)\n",
        "report_fasttext = classification_report(y_test, y_pred_fasttext, output_dict=True)\n",
        "f1_score_fasttext = report_fasttext['accuracy']  # extract accuracy as f1-score\n",
        "\n",
        "# display model performance metrics\n",
        "print(\"\\nmodel performance:\")\n",
        "print(f\"accuracy: {accuracy_fasttext:.4f}\")  # print accuracy with 4 decimal places\n",
        "print(f\"f1 score: {f1_score_fasttext:.4f}\")  # print f1 score with 4 decimal places\n"
      ],
      "metadata": {
        "id": "5bzYzfcJ61ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**s4: analyze false positives (FP) & false negatives (FN)**\n",
        "\n",
        "Now that the FastText classifier's performance has been evaluated (69.55% accuracy), the next step is to analyze its errors.\n",
        "\n",
        "\n",
        "False Positives (FP) → Sentences wrongly classified as positive.\n",
        "\n",
        "False Negatives (FN) → Sentences wrongly classified as negative.\n",
        "\n",
        "Identify patterns in misclassifications to compare with previous models."
      ],
      "metadata": {
        "id": "QUzXhlXT7BSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s4: analyze false positives (FP) & false negatives (FN)\n",
        "\n",
        "# initialize lists to store misclassified cases\n",
        "false_positives_fasttext = []  # sentences incorrectly classified as positive\n",
        "false_negatives_fasttext = []  # sentences incorrectly classified as negative\n",
        "\n",
        "# loop through test data to find misclassified examples\n",
        "for i in range(len(y_test)):\n",
        "    if y_pred_fasttext[i] == 'positive' and y_test.iloc[i] != 'positive':\n",
        "        false_positives_fasttext.append((X_test.iloc[i], y_test.iloc[i], y_pred_fasttext[i]))  # collect FP examples\n",
        "    elif y_pred_fasttext[i] == 'negative' and y_test.iloc[i] != 'negative':\n",
        "        false_negatives_fasttext.append((X_test.iloc[i], y_test.iloc[i], y_pred_fasttext[i]))  # collect FN examples\n",
        "\n",
        "# limit display to first 25 examples in each category\n",
        "num_fp_display = min(25, len(false_positives_fasttext))  # show up to 25 false positives\n",
        "num_fn_display = min(25, len(false_negatives_fasttext))  # show up to 25 false negatives\n",
        "\n",
        "# create a dataframe for FP & FN results\n",
        "fp_fn_df_fasttext = pd.DataFrame({\n",
        "    \"sentence\": [fp[0] for fp in false_positives_fasttext[:num_fp_display]] + [fn[0] for fn in false_negatives_fasttext[:num_fn_display]],\n",
        "    \"true label\": [fp[1] for fp in false_positives_fasttext[:num_fp_display]] + [fn[1] for fn in false_negatives_fasttext[:num_fn_display]],\n",
        "    \"predicted label\": [fp[2] for fp in false_positives_fasttext[:num_fp_display]] + [fn[2] for fn in false_negatives_fasttext[:num_fn_display]]\n",
        "})\n",
        "\n",
        "# print the misclassified sentences\n",
        "print(\"\\nfalse positives & false negatives:\")\n",
        "print(fp_fn_df_fasttext)\n"
      ],
      "metadata": {
        "id": "RY5ELzon7A65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDKo1hg67lgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**final step: interpretation & comparison**\n",
        "\n",
        "fasttext supervised classification achieved 69.\n",
        "\n",
        "55% accuracy, slightly better than logistic regression (68.09%) and LinearSVC (68.09%).\n",
        "\n",
        "\n",
        "to compare performance, a black & gray bar chart will visualize the accuracy of all three models."
      ],
      "metadata": {
        "id": "Q1zbOg0p7nPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final step: enhanced visualization for model comparison\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define model names and their accuracy scores\n",
        "models = [\"LinearSVC + TF-IDF\", \"Logistic Regression + FastText\", \"FastText Supervised\"]\n",
        "accuracy_scores = [68.09, 68.09, 69.55]  # accuracy percentages\n",
        "\n",
        "# create a bar chart (black and gray only) with enhanced visualization\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# draw bars with improved aesthetics\n",
        "bars = plt.bar(models, accuracy_scores, color=['black', 'dimgray', 'gray'], edgecolor='black', linewidth=1.2)\n",
        "\n",
        "# set labels and title\n",
        "plt.ylabel(\"accuracy (%)\")\n",
        "plt.title(\"model performance comparison\")\n",
        "plt.ylim(65, 71)  # keeps scale compact for easier comparison\n",
        "\n",
        "# add text labels on bars with better positioning\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.2, f\"{height:.2f}%\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# add grid lines for better readability\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# show the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A2bhn96-7uaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dr. Yu, sorry for the delay, and thanks for being patient with this one -\n",
        "\n",
        "-just adding a quick thoughts about last week’s homework experience\n",
        "\n",
        "-ai helped with a few things - specifically -  structuring code, fixing syntax, formatting, setting up visualizations, and explaining concepts when things weren’t clear.\n",
        "\n",
        "-it has become to my strategy that - it was useful for breaking things down into smaller, manageable steps, but the big-picture thinking and decision-making were entirely mine - things like model training, evaluation, and interpretation\n",
        "\n",
        "-also, some thought  about technique issues last week\n",
        "\n",
        "-colab lagging, codespaces freezing/crashed - tried different fixes, nothing really helped - ended up upgrading to cloud computing, and instantly solved everything.\n",
        "\n",
        "-not a fan of extra spending, but I guess this one was worth it—saved time\n",
        "\n",
        "-Dujun"
      ],
      "metadata": {
        "id": "l2TMIEyu9-N_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base]",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}